{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><font size = 6>Crawl EdgeProperty Data</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this notebook is to crawl Edge Property data, you can refer this link for more details: [Edgeprop singapore](https://www.edgeprop.sg/property-search?listing_type=sale&property_type=&district=&bedroom_min=&asking_price_min=&asking_price_max=&floor_area_min=&floor_area_max=&land_area_min=&land_area_max=&tenure=&bathroom=&furnishing=&completed=&level=&completion_year_min=&completion_year_max=&rental_yield=&high_rental_volume=&high_sales_volume=&deals=&nearby_amenities=&amenities_distance=500&rental_type=&keyword_features=&keyword=&mrt_keywords=&school_keywords=&asset_id=&resource_type=&x=&y=&radius=1000&search_by=&search_by_distance=&search_by_location=&search_by_showmap=true&below_valuation=&map_zoom=&asset_lat=&asset_lng=&page=1&pageSize=10&order_by=recommended&is_search=true&v360=0&fittings=&saved_search_id=)\n",
    "\n",
    "Since run all the cells again will consume too much time, I cleared all the output in the notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "1. [Import Libraries](#0)<br>\n",
    "2. [Collect all the pages' URL and get all Child links](#1)<br>\n",
    "3. [Crawl Basic Property information from Child links and write to csv](#2) <br>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries <a id=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from requests.exceptions import ConnectionError\n",
    "import csv\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect all the pages' URL and get all Child links <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_url = []\n",
    "for i in range(1,2449): # 2449 subpages in total\n",
    "    url = 'https://www.edgeprop.sg/property-search?listing_type=sale&property_type=&district=&bedroom_min=&asking_price_min=&asking_price_max=&floor_area_min=&floor_area_max=&land_area_min=&land_area_max=&tenure=&bathroom=&furnishing=&completed=&level=&completion_year_min=&completion_year_max=&rental_yield=&high_rental_volume=&high_sales_volume=&deals=&nearby_amenities=&amenities_distance=500&rental_type=&keyword_features=&keyword=&mrt_keywords=&school_keywords=&asset_id=&resource_type=&x=&y=&radius=1000&search_by=&search_by_distance=&search_by_location=&search_by_showmap=true&below_valuation=&map_zoom=&asset_lat=&asset_lng=&page='+str(i)+'&pageSize=10&order_by=recommended&is_search=true&v360=0&fittings=&saved_search_id='\n",
    "    list_url.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_url[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request\n",
    "list_soup = []\n",
    "list_coverpage_properties = []\n",
    "\n",
    "for url in list_url:\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except ConnectionError as e:\n",
    "        print(e)\n",
    "    r.status_code\n",
    "    coverpage = r.content\n",
    "    \n",
    "    soup = BeautifulSoup(coverpage, 'html')\n",
    "    list_soup.append(soup)\n",
    "    \n",
    "    coverpage_properties = soup.find_all('div', class_=\"imgClick\")\n",
    "    list_coverpage_properties.extend(coverpage_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists for content, links and titles\n",
    "\n",
    "list_child_links = []\n",
    "\n",
    "for n in range(0, len(list_coverpage_properties)):\n",
    "    if list_coverpage_properties[n].find('a') is not None:\n",
    "        link = list_coverpage_properties[n].find('a')['href'] \n",
    "        list_child_links.append(link)\n",
    "        # remove duplicates\n",
    "        list_child_links = list(dict.fromkeys(list_child_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of all the pages is:\", len(list_soup) , \"\\n\" \"The number of Child links is:\", len(list_child_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Basic Property information from Child links and write to csv  <a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = []\n",
    "\n",
    "for url in list_child_links:\n",
    "    dic = {}\n",
    "    page=requests.Session().get(url) \n",
    "    tree=html.fromstring(page.text) \n",
    "    try:\n",
    "        title=tree.xpath('//h[@class=\"listing-details columns\"]//span/text()') \n",
    "        price = tree.xpath('//div[@class=\"calculator-content-text text\"]//span/text()')\n",
    "        detail = tree.xpath('//div[@class=\"listing-icon-content\"]//span[@class=\"show\"]/text()') \n",
    "        detail2 = tree.xpath('//div[@class=\"right-content columns\"]//span/text()') \n",
    "        dic['Name']=title\n",
    "        dic['Bed']=detail[0]\n",
    "        dic['Bath']=detail[1]\n",
    "        dic['Type']=detail2[0]\n",
    "        dic['Tenure']=detail2[1]\n",
    "        dic['Psf']=detail2[2]\n",
    "        dic['Floor']=detail2[3]\n",
    "        dic['Top']=detail2[4]\n",
    "        dic['Relisted']=detail2[5]\n",
    "        dic['Furnish']=detail2[6]\n",
    "        dic['Size']=detail2[7]\n",
    "        dic['Price']=price[0]\n",
    "        if len(price)>1 and price[1].rfind('Fair Value')>=0:\n",
    "            dic['Fairvalue']=price[1]\n",
    "        else:\n",
    "            dic['Fairvalue']='-'\n",
    "    except:\n",
    "        print(url)\n",
    "    dict.append(dic)\n",
    "\n",
    "    # print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_columns = ['Address','Postcode','Name','Bed','Bath','Type','Tenure','Psf','Floor','Top','Relisted','Furnish','Size','Price','Fairvalue', 'URL']\n",
    "with open('EdgeProp_Data.csv', 'w', newline='') as csvfile:\n",
    "    fields = ['Address','Postcode', 'Name','Bed','Bath','Type','Tenure','Psf','Floor','Top','Relisted','Furnish','Size','Price','Fairvalue', 'URL']\n",
    "\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in dict:\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
